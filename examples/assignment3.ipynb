{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from milligrad import Tensor, Adam\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Loading and processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, labels = [], []\n",
    "with open(\"data/ascii_names.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        *name, label = line.split(\" \") # *name since people can have names split by spaces like: De Santis\n",
    "        name = \" \".join(name)\n",
    "        names.append(name.upper())\n",
    "        # Append the label to the labels list, converting it to an integer\n",
    "        labels.append(int(label.replace(\"\\n\",\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_chr = {i:c for i,c in enumerate(sorted(set(\"\".join(names))))} # find all unique characters, sort them and give them a unique id\n",
    "\n",
    "# add unique <PAD> token to pad the names to have the same length as the longest name in a batch (cannot construct a Tensor if they have variable lengths)\n",
    "PAD = \".\" # normally <PAD> is used but that requires extra considerations when tokenizing (longest substring), so I just use a space\n",
    "PAD_ID = len(ids_to_chr)\n",
    "ids_to_chr[PAD_ID] = PAD\n",
    "\n",
    "chr_to_ids = {c:i for i,c in ids_to_chr.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_chr.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pad all names to have the same length as the longest name (14 characters)\n",
    "n_len = max(len(n) for n in names)\n",
    "names = [n + \" \"*(n_len-len(n)) for n in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_names = np.array([[chr_to_ids[c] for c in name] for name in names])\n",
    "tokenized_names = np.eye(len(chr_to_ids))[tokenized_names].swapaxes(1,2) # one hot encode and make the length be the last dimension\n",
    "labels = np.array(labels)\n",
    "labels_ohe = np.eye(max(labels))[labels-1] # one hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "\n",
    "N_TRAIN = int(0.7 * len(names))\n",
    "N_VAL = int(0.2 * len(names))\n",
    "N_TEST = len(names) - N_TRAIN - N_VAL\n",
    "\n",
    "shuffle = np.random.permutation(np.arange(len(names)))\n",
    "\n",
    "x_train, y_train = tokenized_names[shuffle[:N_TRAIN]], labels_ohe[shuffle[:N_TRAIN]]\n",
    "x_val, y_val = tokenized_names[shuffle[N_TRAIN:N_TRAIN+N_VAL]], labels_ohe[shuffle[N_TRAIN:N_TRAIN+N_VAL]]\n",
    "x_test, y_test = tokenized_names[shuffle[N_TRAIN+N_VAL:]], labels_ohe[shuffle[N_TRAIN+N_VAL:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS_IN = len(chr_to_ids)\n",
    "CHANNELS_HIDDEN = 64\n",
    "KERNEL_SIZE = 7\n",
    "PADDING = 0 # with stride = 1 and kernel_size = 3, the length of the sequence is preserved\n",
    "\n",
    "SEQ_LEN = n_len\n",
    "NUM_CLASSES = max(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3-0.4 Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not have special considerations for the sparsity of the data since my aim was to make a generalized convolution \n",
    "class SurnameConvNet:\n",
    "    def __init__(\n",
    "            self, c_in, c_hidden, kernel_size, padding,\n",
    "            orig_seq_len, num_classes\n",
    "        ):\n",
    "        self.c_in, self.c_hidden = c_in, c_hidden\n",
    "        self.kernel_size, self.padding = kernel_size, padding\n",
    "        self.w_out = (orig_seq_len - kernel_size + 2*padding + 1) - kernel_size + 2*padding + 1\n",
    "\n",
    "        self.k1 = Tensor.randn(c_in, kernel_size, c_hidden) * 0.01\n",
    "        self.b1 = Tensor.zeros(c_hidden, 1)  # 1 to broadcast over sequence\n",
    "        self.k2 = Tensor.randn(c_hidden, kernel_size, 16) * 0.01\n",
    "        self.b2 = Tensor.zeros(16, 1)\n",
    "\n",
    "        self.w = Tensor.xavier(self.w_out * 16, num_classes)\n",
    "        self.b = Tensor.zeros(num_classes)\n",
    "\n",
    "    def __call__(self, x:Tensor)->Tensor:\n",
    "        x = (x.conv1d(self.k1, padding=self.padding) + self.b1).relu()\n",
    "        x = (x.conv1d(self.k2, padding=self.padding) + self.b2).relu()\n",
    "        x = x.reshape(-1, self.w_out * 16) # squeeze away unary dimension (k2 has one filter)\n",
    "        return x @ self.w + self.b\n",
    "    \n",
    "    def parameters(self)->list[Tensor]:\n",
    "        return [self.k1, self.b1, self.k2, self.b2, self.w, self.b]\n",
    "\n",
    "\n",
    "model = SurnameConvNet(CHANNELS_IN, CHANNELS_HIDDEN, KERNEL_SIZE, PADDING, SEQ_LEN, NUM_CLASSES)\n",
    "model(Tensor.randn(32, CHANNELS_IN, SEQ_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "\n",
    "model = SurnameConvNet(CHANNELS_IN, CHANNELS_HIDDEN, KERNEL_SIZE, PADDING, SEQ_LEN, NUM_CLASSES)\n",
    "optim = Adam(model.parameters())\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_shuffle = np.random.permutation(len(x_train))\n",
    "    train_tqdm = tqdm(range(0, len(x_train) // BATCH_SIZE - 1), desc=f\"Epoch {epoch + 1}/{EPOCHS} Training\")\n",
    "    for i in train_tqdm:\n",
    "        idxs = train_shuffle[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "\n",
    "        x = Tensor(x_train[idxs])\n",
    "        y = Tensor(y_train[idxs])\n",
    "\n",
    "        y_hat = model(x)\n",
    "\n",
    "        loss = -(y * y_hat.log_softmax()).sum(-1).mean() # sum over classes, mean over batch\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        train_tqdm.set_postfix({\"loss\": loss.data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    return (y.argmax(axis=-1)==y_hat.argmax(axis=-1)).mean()\n",
    "\n",
    "accuracy(y_train, model(Tensor(x_train)).data), accuracy(y_test, model(Tensor(x_test)).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from milligrad import topological_sort\n",
    "\n",
    "[x._grad_fn for x in reversed(topological_sort(loss))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from milligrad.tensor import broadcast_to\n",
    "\n",
    "a = Tensor.randn(32, 3, 8, 3)\n",
    "b = Tensor.randn(8,1)\n",
    "\n",
    "a+b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
